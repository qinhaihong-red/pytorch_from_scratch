{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda as cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. cuda包：torch.cuda\n",
    "\n",
    "此包为CUDA张量类型增加了支持，即实现了与CPU张量一样的函数，只是为算力而利用了GPU.\n",
    "\n",
    "这个包总是懒初始化的，因此总是可以引入它. 通过使用is_available来判断系统是否支持CUDA.\n",
    "\n",
    "阅读 CUDA语义 了解更多细节：https://pytorch.org/docs/stable/notes/cuda.html#cuda-semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.current_blas_handle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.current_device():返回当前所选设备的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index=cuda.current_device()\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.device_count():返回当前可用的GPU数目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num=cuda.device_count()\n",
    "num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.get_device_capability(device): 返回设备的cuda能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.get_device_capability(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.get_device_name(device):返回设备名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 960M'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.max_memory_allocated(device):返回指定设备张量的最大GPU内存用量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.max_memory_allocated(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device('cuda') if cuda.is_available() else torch.device('cpu')\n",
    "X=torch.randn(100,100,device=device)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "905728"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.max_memory_allocated(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.max_memory_cached(device=None):返回指定设备缓存分配器管理的最大GPU内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.max_memory_cached(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.memory_allocated(device=None):返回指定设备上张量使用的GPU内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "905728"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.memory_allocated(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.memory_cached(device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  orch.cuda.set_device(device):设置当前的设备. 该函数不鼓励使用. 更好的方法为使用CUDA_VISIBLE_DEVICES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.stream(stream):用于选定流的上下文管理器."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.synchronize():在当前设备上等待所有流中核操作的结束."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.随机数生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.get_rng_state(device=-1):返回当前GPU的随机数生成器状态."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.set_rng_state(new_state, device=-1):设置当前GPU的随机数生成器状态."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.manual_seed(seed):为当前GPU设置生成随机数的种子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.manual_seed_all(seed):为多GPU设置生成随机数的种子."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.seed():为当前GPU设置随机数种子."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.seed_all():为多GPU设置随机数种子."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.initial_seed():返回当前GPU的种子."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 通信集体 Communication Collectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.comm.broadcast(tensor, devices):把一个张量广播到给定数量的GPU上."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760):把指定的张量序列广播到指定设备上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.comm.reduce_add(inputs, destination=None):对多GPU上的张量求和."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.comm.scatter(tensor, devices, chunk_sizes=None, dim=0, streams=None): 把张量分散到多GPU上."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.comm.gather(tensors, dim=0, destination=None):把多GPU上的张量进行聚集."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.流和事件 Streams and Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 class torch.cuda.Stream\n",
    "\n",
    "对一个CUDA stream的封装.\n",
    "\n",
    "stream是属于特定设备的、由线性可执行操作组成的序列，与其他的流独立.\n",
    "\n",
    "类似一个支持并发的队列.\n",
    "\n",
    "__方法：__\n",
    "\n",
    "- query:所有提交的任务是否完成\n",
    "\n",
    "\n",
    "- record_event:\n",
    "\n",
    "\n",
    "- synchronize:\n",
    "\n",
    "\n",
    "- wait_event:\n",
    "\n",
    "\n",
    "- wait_stream:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 class torch.cuda.Event\n",
    "\n",
    "对CUDA事件的封装类.\n",
    "\n",
    "- elapsed_time:\n",
    "\n",
    "\n",
    "- ipc_handle:\n",
    "\n",
    "\n",
    "- query:\n",
    "\n",
    "\n",
    "- record:\n",
    "\n",
    "\n",
    "- synchronize:\n",
    "\n",
    "\n",
    "- wait:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 内存管理\n",
    "\n",
    "- torch.cuda.empty_cache():释放所有当前被缓存分配器持有的未被使用的缓存内存，使得这些被释放的内存可被其他GPU应用程序使用，并在\n",
    "nvidia-smi中被发现.\n",
    "\n",
    "注意这个函数不会增加Pytorch的GPU内存可用量. 见：https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 其他memory_allocated和memory_cached见上述，不再赘述."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. nvidia 工具扩展（NVTX）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cuda.nvtx.mark(msg)\n",
    "\n",
    "\n",
    "- torch.cuda.nvtx.range_push(msg)\n",
    "\n",
    "\n",
    "- torch.cuda.nvtx.range_pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
