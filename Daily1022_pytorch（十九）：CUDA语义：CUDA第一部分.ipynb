{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as tf\n",
    "import torchvision.models as mod\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA语义\n",
    "\n",
    "- 通过使用torch.cuda来创建和运行CUDA操作. 它会追踪当前所选的GPU，所有分配的CUDA张量默认都会创建在这个GPU设备上.\n",
    "\n",
    "\n",
    "- 可以通过torch.cuda.device上下文管理器改变当前所选的设备.\n",
    "\n",
    "\n",
    "- 一旦张量被分配，可以在上面做任何操作，而不用考虑所选的设备，并且所有的结果都会像之前被操作的张量一样，置于同样的设备上.\n",
    "\n",
    "\n",
    "- 跨GPU操作默认是不被允许的，除了copy\\_()方法以及类似的函数，如to()和cuda()函数.\n",
    "\n",
    "\n",
    "- 除非允许点到点的内存存取，否则在任何跨设备的张量上进行操作都会引发错误.\n",
    "\n",
    "CUDA示例如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda=torch.device('cuda')\n",
    "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "cpu=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda0=torch.device('cuda:0')\n",
    "cuda0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##这些超过0的GPU序数，尽管可以定义，但是不能使用\n",
    "##因为只有一个GPU\n",
    "\n",
    "cuda1=torch.device('cuda:1')\n",
    "cuda1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda2=torch.device('cuda:2')\n",
    "cuda2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分配张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=torch.arange(3)\n",
    "y=torch.arange(3,6,device='cuda')\n",
    "z=torch.arange(7,10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2=torch.full((3,),fill_value=1,dtype=torch.long)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8, 9], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##两个cpu上的张量相加\n",
    "ret1=x+x2\n",
    "ret1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.LongTensor but found type torch.cuda.LongTensor for argument #3 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-80b18617ba08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m##类型不匹配，导致cpu上的tensor与gpu上的tensor无法直接相加\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mret2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mret2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of type torch.LongTensor but found type torch.cuda.LongTensor for argument #3 'other'"
     ]
    }
   ],
   "source": [
    "##类型不匹配，导致cpu上的tensor与gpu上的tensor无法直接相加\n",
    "ret2=x+y\n",
    "ret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 12, 14], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##两个GPU上的张量相加\n",
    "##结果仍然为GPU上的张量\n",
    "ret3=y+z\n",
    "ret3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  互转"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1189, -0.6166])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_cpu=torch.randn(2)\n",
    "y1_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1189, -0.6166], device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_gpu=y1_cpu.to(device)\n",
    "y1_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1189, -0.6166], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_gpu=y1_cpu.cuda('cuda')\n",
    "y1_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1189, -0.6166])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_cpu=y1_gpu.to('cpu')\n",
    "y1_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 异步执行\n",
    "\n",
    "GPU操作默认是异步执行的。当调用一个使用GPU的函数时，该操作被入队于特定设备，但是并非直到之后才执行. 这允许我们可以并行地执行更多计算，\n",
    "包括CPU和其他GPU上的操作.\n",
    "\n",
    "一般来说，异步调用的效果对调用者是不可见的，因为：\n",
    "\n",
    "- 每个入队设备的操作，以入队顺序被设备执行\n",
    "\n",
    "- 当在CPU和GPU之间或者GPU与GPU之间进行拷贝数据时，Pytorch会自动执行必备的同步操作\n",
    "\n",
    "因此计算将会持续进行，犹如每个操作都是同步执行的一样.\n",
    "\n",
    "通过设置环境变量 CUDA_LAUNCH_BLOCKING=1 ，可以强制使用同步计算. 当在GPU上发生错误时，这个操作可能很有用.\n",
    "(因为当异步执行时，直到操作被执行后发生错误时，才会报告错误，此时堆栈追踪并不会显示是哪里导致的错误)\n",
    "\n",
    "作为一个例外，有些函数，如copy\\_(), 会使用额外的aync参数，能让调用者在不必要的时候绕过同步操作。另个例外是CUDA流（streams），如下所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.CUDA流\n",
    "\n",
    "一个CUDA stream是属于一个特定设备的、由线性操作操作构成的序列.\n",
    "\n",
    "一般没有必要显示创建一个：每个设备有一个默认的流.\n",
    "\n",
    "每个流内部的操作以它们被创建的顺序进行序列化，但是不同流内部的不同操作，能以任何相对顺序并发执行，除非使用显式的同步函数（如synchronize或wait_stream）\n",
    "\n",
    "\n",
    "在当前流是默认流的情况下，Pytorch会在数据进行移动时，自动执行必备的同步操作. 而如果使用自定义流，那么用户需要自己来保证正确的同步操作."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 内存管理\n",
    "\n",
    "Pytorch使用一个缓存内存的分配器来加速内存分配操作. 这允许快速进行内存回收而不必借助设备同步. \n",
    "\n",
    "然而，如果使用nvidia-smi，由分配器管理的未使用的内存，仍然会显示在其中.\n",
    "\n",
    "可以使用memory_allocated和max_memory_allocated来监视被张量占用的内存，同时使用memory_cached和max_memory_cached来监视\n",
    "被缓存分配器管理的内存.\n",
    "\n",
    "调用empty_cache可以释放Pytorch中未使用的缓存内存，这样这些内存可被其他GPU应用使用.\n",
    "\n",
    "但是被张量占用的内存将不会被释放."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 最佳实践\n",
    "\n",
    "### 4.1 使用设备无关的代码\n",
    "\n",
    "\n",
    "主要时通过检测是否有可用的GPU，通过调节判断，给出一个device，如：b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device= torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##通过使用new_xx方法，可以创建使用同类型设备的张量\n",
    "x_cpu=torch.randn(2)\n",
    "y_gpu=torch.randn(2,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.],\n",
       "        [2., 2., 2.]], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_gpu2=y_gpu.new_full((2,3),fill_value=2)\n",
    "y_gpu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##以及zeros_like或ones_like\n",
    "y_gpu3=torch.ones_like(y_gpu2)\n",
    "y_gpu3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 使用钉住的内存(pinned memory)缓存\n",
    "\n",
    "如果数据源于钉住的内存（锁页内存），那么从主机到GPU的拷贝将会快得多.\n",
    "\n",
    "CPU张量和存储，暴露了一个pin_memory的方法，该方法返回对象的拷贝，而该拷贝位于一个钉住的区域.\n",
    "\n",
    "一旦钉住一个张量或存储，就可以使用异步的GPU拷贝. 只需要在调用cuda()的时候，额外传入一个non_blocking=True的参数. \n",
    "可以在重叠数据转移计算中使用.\n",
    "\n",
    "通过给DataLoader的构造函数传入pin_memory=True的参数，可以使得DataLoader返回位于钉住内存区域的批数据."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 使用nn.DataParallel进行数据并行而非使用多进程 multiprocessing\n",
    "\n",
    "多数涉及到批输入和多GPU的用例，应该默认使用DataParallel来利用更多的GPU。\n",
    "\n",
    "如果在multiprocessing中使用CUDA模型，会有许多重要的警告. 如果未按需要来确切处理数据, 很可能你的程序会遇到不正确或未定义的行为."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
