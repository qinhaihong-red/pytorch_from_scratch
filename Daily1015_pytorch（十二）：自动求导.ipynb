{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.局部 禁止/恢复 求导\n",
    "\n",
    "在某些情况下，比如进行推断时，没有必要进行求导。同时可以节约内存。\n",
    "\n",
    "可使用的方式如下：\n",
    "- __上下文管理器方式__：torch.no_grad/torch.enable_grad\n",
    "- __函数方式__：set_grad_enabled. 这个函数也可以用作上下文管理器."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##1.1 torch.no_grad 禁止求导的上下文管理器\n",
    "x=torch.randn(3,requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y=x*2\n",
    "    \n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##对于函数，可以使用装饰器修饰\n",
    "@torch.no_grad()\n",
    "def doubler(x):\n",
    "    return x*2\n",
    "\n",
    "z=doubler(x)\n",
    "z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##1.2 torch.enable_grad 恢复求导的上下文管理器\n",
    "##这个管理器可以在禁止求导管理器的内部，恢复求导，但是不会影响到外部\n",
    "with torch.no_grad():\n",
    "    with torch.enable_grad():\n",
    "        y=x*2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.enable_grad()\n",
    "def doubler2(x):\n",
    "    return x*2\n",
    "\n",
    "with torch.no_grad():\n",
    "    z=doubler2(x)\n",
    "    \n",
    "z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##1.3 禁止/恢复求导函数\n",
    "torch.set_grad_enabled(False)\n",
    "y=x*2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "z=x*2\n",
    "z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##也可以用作上下文管理器\n",
    "with torch.set_grad_enabled(False):\n",
    "    y=x*2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x2209a2ca080>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.2477, 1.1540]) \n",
      " tensor([2.2477, 1.1540]) \n",
      " None\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "t=torch.randn(2)\n",
    "print(t,'\\n',t.data,'\\n',t.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2269, 0.0761], requires_grad=True) \n",
      " tensor([0.2269, 0.0761]) \n",
      " None\n"
     ]
    }
   ],
   "source": [
    "t2=torch.randn(2,requires_grad=True)\n",
    "print(t2,'\\n',t2.data,'\\n',t2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=x*2\n",
    "z=y.sum()\n",
    "z.backward()\n",
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.自动求导机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 使用requires_grad从反向传播中排除不需计算的子图\n",
    "\n",
    "\n",
    "\n",
    "- 如果对操作的任一输入参数需要梯度，那么输出也要求梯度.\n",
    "就是说，当所有的输入参数都不需要求梯度的时候，那么输出也就不需要求梯度了，这时反向计算过程也就不会执行.\n",
    "\n",
    "\n",
    "- 使用requires_grad可以从梯度计算中排除具有良好粒度的、不需要计算梯度的子图，这样能提升运行效率."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Autograd:逆向自动微分系统\n",
    "\n",
    "####  理论概念：DAG计算图\n",
    "- 通过记录已创建的数据以及在这些数据上执行的操作，autograd得到了一张有向无环图（DAG）\n",
    "- 这张图的叶子（leves）节点是输入张量，根节点（root）是输出张量，中间层有临时节点和算子节点\n",
    "- 通过对这张图从根节点到叶节点的追踪，使用链式法则（chain-rule），可以求得对应叶子节点的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 内部实现：Function和grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- autograd在内部通过Function对象来表示DAG计算图. 即通过调用Function对象的apply函数来计算结果，以求得图的值.\n",
    "- 在计算前向传递时，autograd同时执行所请求的计算并建立表示计算函数的图，而torch.Tensor的属性grad_fn则是这张图的入口点\n",
    "- 当前向传递结束后，在反向传递中计算图的相关梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 动态图优势：如何运行即如何微分\n",
    "\n",
    "- 每次迭代，图都会从头开始重建. 这意味着在每次迭代中，都可以使用改变图全部形状和尺寸的任意python控制流表达式.\n",
    "- 在开始训练前，不必编码所有可能的路径，就是说:如何运行就如何微分."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 autograd的就地操作\n",
    "\n",
    "由于autograd的缓存释放与重用机制已经非常高效，所以除非面临非常大的内存压力，否则不鼓励在autograd中使用就地操作.\n",
    "\n",
    "限制就地操作适用性的原因如下：\n",
    "\n",
    "- 就地操作可以潜在覆写(overwrite)需要求梯度的变量值\n",
    "- 每个就地操作实际上都会要求实现对计算图的重写(rewrite)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 就地正确性检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 每个tensor都有一个版本计数器，任何导致该tensor变脏的操作，都会使得计数器增加.当Function对象为反向传递保存任何tensor时，这些tensor的计数器也会保存.\n",
    "\n",
    "\n",
    "- 每次访问self.saved_tensor都将触发检测，如果发现比保存的值大，将抛出异常.这将会确保在使用就地函数时，不发生任何错误并保证梯度计算的正确性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Tensor类的autograd函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过链式法则对图进行微分，计算当前张量关于叶子节点的梯度\n",
    "- 如果当前张量是非标量（即含有多于一个元素的张量），计算梯度时需要额外指定gradient参数（看下面的示例）\n",
    "- 该函数对叶子节点求梯度会进行累积，如不需要可在调用前对叶子节点已存在的梯度进行清零"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 标量梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(2,4,dtype=torch.float,requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 6.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=x**2\n",
    "z=y.sum()\n",
    "z.backward()##retain_graph=False\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x.grad)##梯度仍然是Tensor类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify >retain_graph=True when calling backward the first time.\n",
    "\n",
    "__retain_graph=False，因此当反向传递通过图之后，缓存会被释放. 无法进行第二次调用.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 6.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()##如不清零，x的grad就会累积为：8, 12\n",
    "y=x**2\n",
    "z=y.sum()\n",
    "z.backward(retain_graph=True)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8., 12.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.backward()##因为设置了retain_graph=True，因此这里可以再调用一次\n",
    "x.grad##梯度累积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##z.backward() ##retain_graph的生命期只会延续到下一次调用，然后被置为False，导致再次调用抛出异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 14.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=x*2\n",
    "z=y.sum()\n",
    "z.backward()\n",
    "x.grad##梯度一直在累积，尽管对应了不同函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##对标量值函数反向传递指定gradient\n",
    "'''\n",
    "x.grad.zero_()\n",
    "y=x**2\n",
    "z=y.sum()\n",
    "z.backward(gradient=torch.tensor([1,1],dtype=torch.float))\n",
    "x.grad\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">RuntimeError: invalid gradient at index 0 - expected shape [] but got [2]\n",
    "\n",
    "__结论：对于标量函数，gradient不能指定，只能为None__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 多变量函数Jacobian矩阵(  $ f:R^{m}\\rightarrow R^{n} $  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 线性函数\n",
    "x.grad.zero_()\n",
    "y=2*x##y1=2*x1,y2=2*x2\n",
    "y.backward(gradient=torch.ones(2,dtype=torch.float))##dy1/dx1=2,dy2/dx2=2\n",
    "x.grad##求得的结果相当于y的Jacobian矩阵的各列向量之和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 0.])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y=2*x\n",
    "y.backward(gradient=torch.tensor([1,0],dtype=torch.float))##对于线性函数，这里不指定retain_grad也可以\n",
    "x.grad##dy1/dx1=2,dy1/dx2=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2.])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.backward(gradient=torch.tensor([0,1],dtype=torch.float))\n",
    "x.grad##dy1/dx1=2,dy1/dx2=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__上面两个合起来就是Jacobian矩阵了__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13., 11., 12.]], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 非线性函数\n",
    "z=torch.zeros(3,dtype=torch.float).reshape(1,-1)\n",
    "z[0,0]=x[0]**2+3*x[1]##使用x的元素为z赋值，z的req_grad仍然为True\n",
    "z[0,1]=x[1]**2+x[0]\n",
    "z[0,2]=2*x[0]*x[1]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11., 13.])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "z.backward(gradient=torch.ones(3).reshape(1,3),retain_graph=True)##这里需要指定retain_grad，否则无法多次backward\n",
    "x.grad##这个grad是下面的Jacobian矩阵的各列向量的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 3.])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "z.backward(gradient=torch.Tensor([[1,0,0]]),retain_graph=True)\n",
    "j0=torch.tensor(x.grad)##注意：需要使用tensor包裹一下grad再赋值给j0,这是copy赋值.否则就是引用，到最后Jacobian矩阵的各行向量都一样了\n",
    "j0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 6.])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "z.backward(gradient=torch.Tensor([[0,1,0]]),retain_graph=True)\n",
    "j1=torch.tensor(x.grad)\n",
    "j1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 4.])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "z.backward(gradient=torch.Tensor([[0,0,1]]),retain_graph=True)\n",
    "j2=torch.tensor(x.grad)\n",
    "j2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 3.],\n",
       "       [1., 6.],\n",
       "       [6., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j=np.vstack((j0.numpy(),j1.numpy(),j2.numpy()))\n",
    "j##Jacobian矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 4.])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "z.backward(gradient=torch.Tensor([[0,0,1]]))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "x.grad.zero_()\n",
    "z.backward(gradient=torch.Tensor([[0,0,1]]))\n",
    "x.grad\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__由于上面已经取消了retain_grad，因此这里再次backward会抛出异常__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 detach和detach_\n",
    "\n",
    "- detach: 返回当前张量的引用\n",
    "- detach\\_: 使得当前张量脱离创建它的图，变成一个叶子节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.grad.zero_()\n",
    "y=2*x\n",
    "z=y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 6.])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.detach_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.detach_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "\n",
    "__注意：detach之后，变成一个叶子节点，无法进行反向传递__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=y.sum()##y已经是叶子了，对它的操作输出到z，z仍然是叶子\n",
    "z.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##先输出操作到z，再detach\n",
    "y=2*x\n",
    "z=y.sum()\n",
    "z.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.detach_()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2.])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##先detach_，再输出操作到z，此时z也变成了叶子\n",
    "y=2*x\n",
    "y.detach_()\n",
    "z=y.sum()\n",
    "z.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3.])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new=x.detach()##detach相当于返回tensor数据的引用\n",
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 5.], requires_grad=True)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new[1]=5##对传出的变量进行修改，也会影响到原变量\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=2*x\n",
    "z=y.sum()\n",
    "y.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new=y.detach()\n",
    "y.is_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 register_hook\n",
    "\n",
    "- 对中间变量登记该函数，可获得前面变量关于它的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad of z w.r.t y: tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y=2*x\n",
    "z=y.sum()\n",
    "h = y.register_hook(lambda g:print('grad of z w.r.t y:',g))##这个梯度是z关于y的\n",
    "z.backward()\n",
    "h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad of z w.r.t x: tensor([2., 2.])\n"
     ]
    }
   ],
   "source": [
    "print('grad of z w.r.t x:',x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 retain_grad\n",
    "\n",
    "- 对于非叶子节点，反向传递之后，并不会保存后续节点关于它的梯度信息\n",
    "- retain_grad可以使得非叶子节点再反向传递之后保留梯度信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.grad.zero_()\n",
    "y=2*x\n",
    "z=y.sum()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2.]) \n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad,'\\n',y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2.]) \n",
      " tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y=2*x·\n",
    "z=y.sum()\n",
    "y.retain_grad()\n",
    "z.backward(retain_graph=True)\n",
    "print(x.grad,'\\n',y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2.]) \n",
      " tensor([2., 2.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()##对x累积的梯度进行清理\n",
    "z.backward(retain_graph=True)\n",
    "print(x.grad,'\\n',y.grad)##y也会累积梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2.]) \n",
      " tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.grad.zero_()\n",
    "z.backward()\n",
    "print(x.grad,'\\n',y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. autograd.Function对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function对象记录所有操作历史，并为微分计算定义公式. 具体如下：\n",
    "\n",
    "- 在Tensor上执行的每个操作，都会创建一个新的Function对象, 这个对象会执行计算并记录所发生的操作. 所有历史信息保持在由Function对象组成的有向无环图（DAG）中，在这个图上，边（edge）表示依赖关系，比如：input <- output. 然后，当反向传递被调用时，图通过调用每个Function对象的backward()方法，以拓扑序被处理. 然后，把返回的梯度传给下一个Function对象.\n",
    "\n",
    "\n",
    "- 通常，用户与Function打交道的唯一方式是创建一个Function子类并定义新的计算操作. 这是扩展autograd的推荐方式.\n",
    "\n",
    "\n",
    "- 每个函数对象只被使用一次（在前向传递过程中）."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Function的backward和forward方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- static forward(ctx, *args, **kwargs): __保存反向传递所需的参数；执行计算并输出__\n",
    "\n",
    "    - 执行计算操作. 该函数需要被所有子类重写（override）.\n",
    "   \n",
    "    - 需要以一个context对象ctx作为第一个参数，接着可以是任意个其他参数（tensor或者其他类型）\n",
    "\n",
    "    - context对象用来存储tensor对象.并且可以在反向传递过程中，取回这些参数.\n",
    "    \n",
    "    \n",
    "    \n",
    "- static backward(ctx, *grad_outputs)：__读取正向传递保存的参数；计算梯度__\n",
    "\n",
    "    - 为计算微分定义公式. 该函数需要被所有子类重写（override）.\n",
    "    \n",
    "    - 必须以一个context对象作为首个参数，接着是任意个forward()函数返回的值. 并且该函数的返回的tensor个数，需要与传入forward()函数的参数的个数一致. 每个输入参数都是关于给定输出的梯度，每个返回值都是关于相应输入的梯度.\n",
    "    \n",
    "    - ctx对象可以取回在前向传递中保存的tensor. 它同时有个 ctx.needs_input_grad 属性，该属性是Bool类型的tuple，表示哪个输入需要梯度.\n",
    "    例如，如果第一个输入forward函数的参数需要关于输出计算梯度，那么在backward函数就会有：ctx.needs_input_grad[0] = True ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as AT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "##自定义乘法\n",
    "\n",
    "class Mul(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,x,y):\n",
    "        r=ctx.save_for_backward(x,y)\n",
    "        print('in forward\\n')\n",
    "        return x*y\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):##这个grad_output就是外部调用backward时，传入的gradient\n",
    "        print('in backward. grad_output is:',grad_output)\n",
    "        x,y=ctx.saved_tensors\n",
    "        t=torch.empty(2).fill_(y[0])\n",
    "        return t,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in forward\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4., 6.], grad_fn=<MulBackward>)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(2,4,dtype=torch.float,requires_grad=True)\n",
    "##调用算子：正向传递输出计算结果\n",
    "z=Mul.apply(x,torch.tensor([2],dtype=torch.float))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in backward. grad_output is: tensor([1.1000, 2.2000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2., 2.])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##反向传播：计算梯度\n",
    "z.backward(torch.tensor([1.1,2.2]),retain_graph=True)##这个参数就是grad_output\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 自定义平方\n",
    "class sq(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return (x**2).sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        x,=ctx.saved_tensors##注意saved_tensors传回的是个tuple.进行分解之后，再使用.\n",
    "        print('in backward, grad_output is:',grad_output)\n",
    "        return 2*grad_output*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in backward, grad_output is: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y=sq.apply(x)\n",
    "y.backward()##不输入gradient参数，则默认为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 6.])"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
