{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 参数类型Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.3616,  0.6998,  0.1822], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Paramter是tensor的子类\n",
    "## 默认添加了 requires_grad 为 True\n",
    "p=nn.Parameter(torch.randn(3))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3616,  0.6998,  0.1822])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.grad_fn is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7231,  1.3996,  0.3643])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=(p**2).sum()\n",
    "z.backward()\n",
    "p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Parameter容器（一）：ParameterList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterList(\n",
       "    (0): Parameter containing: [torch.FloatTensor of size 1]\n",
       "    (1): Parameter containing: [torch.FloatTensor of size 2]\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##初始化或者为None,或者为一个可推断的迭代器\n",
    "pl=nn.ParameterList([nn.Parameter(torch.randn(i)) for i in range(1,3)])\n",
    "pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.3579], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-1.7983,  0.8289], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterList(\n",
       "    (0): Parameter containing: [torch.FloatTensor of size 1]\n",
       "    (1): Parameter containing: [torch.FloatTensor of size 2]\n",
       "    (2): Parameter containing: [torch.FloatTensor of size 3]\n",
       "    (3): Parameter containing: [torch.FloatTensor of size 4]\n",
       "    (4): Parameter containing: [torch.FloatTensor of size 5]\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##添加与扩展\n",
    "pl.append(nn.Parameter(torch.randn(3)))\n",
    "pl.extend([nn.Parameter(torch.randn(i)) for i in range(4,6)])\n",
    "pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Parameter容器（二）：ParameterDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterDict(\n",
       "    (a): Parameter containing: [torch.FloatTensor of size 0]\n",
       "    (b): Parameter containing: [torch.FloatTensor of size 1]\n",
       "    (c): Parameter containing: [torch.FloatTensor of size 2]\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##初始化或者为None，或者为可迭代的推断\n",
    "pd=nn.ParameterDict({a:nn.Parameter(torch.randn(i)) for i,a in enumerate('abc')})\n",
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##添加\n",
    "pd['d']=nn.Parameter(torch.randn(ord('d')-ord('a')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterDict(\n",
       "    (a): Parameter containing: [torch.FloatTensor of size 0]\n",
       "    (b): Parameter containing: [torch.FloatTensor of size 1]\n",
       "    (c): Parameter containing: [torch.FloatTensor of size 2]\n",
       "    (d): Parameter containing: [torch.FloatTensor of size 3]\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['a', 'b', 'c', 'd'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_values([Parameter containing:\n",
       "tensor([], requires_grad=True), Parameter containing:\n",
       "tensor([-0.3087], requires_grad=True), Parameter containing:\n",
       "tensor([-0.1636, -1.9639], requires_grad=True), Parameter containing:\n",
       "tensor([-0.0884, -2.0917, -0.7969], requires_grad=True)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##更新\n",
    "pd.update({a:nn.Parameter(torch.randn(i)) for i,a in enumerate('efg')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterDict(\n",
       "    (a): Parameter containing: [torch.FloatTensor of size 0]\n",
       "    (b): Parameter containing: [torch.FloatTensor of size 1]\n",
       "    (c): Parameter containing: [torch.FloatTensor of size 2]\n",
       "    (d): Parameter containing: [torch.FloatTensor of size 3]\n",
       "    (e): Parameter containing: [torch.FloatTensor of size 0]\n",
       "    (f): Parameter containing: [torch.FloatTensor of size 1]\n",
       "    (g): Parameter containing: [torch.FloatTensor of size 2]\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有神经网络的模块和层的基类，可以通过它嵌套构成树状结构。\n",
    "\n",
    "#### 重要成员：\n",
    "-  _parameters:OerderedDict类型，value类型是Parameter. 登记在本模块的参数.\n",
    "-  _buffers:OerderedDict类型，value类型是torch.Tensor. 登记在本模块供持久化的缓存.\n",
    "-  _modules:OerderedDict类型，value类型是Module. 添加在本模块的子模块.\n",
    "-  _backward_hooks\n",
    "-  _forward_pre_hooks\n",
    "-  _forward_hooks\n",
    "\n",
    "### 重要函数：\n",
    "- forward:任何继承类都需要重写的函数\n",
    "- register_parameter\n",
    "- register_buffer\n",
    "- add_module\n",
    "- register_backward_hook\n",
    "- register_forkward_hook\n",
    "- register_forkward_pre_hook\n",
    "- parameter/named_parameter：递归yield本模块和所有子模块的所有参数\n",
    "- children/named_children：yield本模块的所有子模块\n",
    "- modules/named_modules：递归yield本模块和所有子模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##4.1 个卷积层+1个最大池化+2个ReLU激活的神经网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1=nn.Conv2d(3,16,3,padding=1)#输入3个通道，输出16个通道，核为3，padding为1，计算可得卷积后图片尺寸不变\n",
    "        self.pool2=nn.AdaptiveMaxPool2d(8)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.conv1(x))\n",
    "        return F.relu(self.pool2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input=torch.randn(20*3*64*64).reshape(20,3,64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16, 8, 8])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=Net()\n",
    "out=n(input)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "        0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##4.2 apply(fn)\n",
    "##对所有子模块（直属的子模块，不递归）施加fn的作用，一般用以初始化\n",
    "def init(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        m.weight.data.fill_(0.1)\n",
    "        m.bias.data.fill_(0.1)\n",
    "\n",
    "n2=Net()\n",
    "n2.apply(init)\n",
    "n2._modules['conv1'].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "AdaptiveMaxPool2d(output_size=8)\n"
     ]
    }
   ],
   "source": [
    "##4.3 children\n",
    "##children只迭代直接的子模块\n",
    "for m in n2.children():\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 --> Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "pool2 --> AdaptiveMaxPool2d(output_size=8)\n"
     ]
    }
   ],
   "source": [
    "##4.4 named_children\n",
    "for n,m in n2.named_children():\n",
    "    print(n,'-->',m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): AdaptiveMaxPool2d(output_size=8)\n",
      ")\n",
      "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "AdaptiveMaxPool2d(output_size=8)\n"
     ]
    }
   ],
   "source": [
    "##4.5 modules\n",
    "##递归所有的模块，包括子模块\n",
    "for m in n2.modules():\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Net(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): AdaptiveMaxPool2d(output_size=8)\n",
      ")\n",
      "conv1 --> Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "pool2 --> AdaptiveMaxPool2d(output_size=8)\n"
     ]
    }
   ],
   "source": [
    "##4.6 named_modules\n",
    "for name,m in n2.named_modules():\n",
    "    print(name,'-->',m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 3, 3])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "##4.7 parameters\n",
    "for p in n2.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight --> torch.Size([16, 3, 3, 3])\n",
      "conv1.bias --> torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "##4.8 named_parameters\n",
    "for n,p in n2.named_parameters():\n",
    "    print(n,'-->',p.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##登记一个参数\n",
    "n2.register_parameter('param1',nn.Parameter(torch.randn(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param1 --> torch.Size([3])\n",
      "conv1.weight --> torch.Size([16, 3, 3, 3])\n",
      "conv1.bias --> torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for n,p in n2.named_parameters():\n",
    "    print(n,'-->',p.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0036, -1.6693, -0.6486])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2._parameters['param1'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##4.9 register_backward_hook\n",
    "def b_hook(m,grad_in,grad_out):\n",
    "    print('backward hook called')\n",
    "\n",
    "n2.register_backward_hook(b_hook)\n",
    "out=n2(input)\n",
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward hook called\n"
     ]
    }
   ],
   "source": [
    "z=out.sum()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##4.10 登记一个buffer\n",
    "n2.register_buffer('buf1',torch.randn(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward hook called\n"
     ]
    }
   ],
   "source": [
    "##4.11 register_forward_hook\n",
    "def f_hook(m,input,out):\n",
    "    print('forward hook called')\n",
    "    \n",
    "n2.register_forward_hook(f_hook)    \n",
    "out2=n2(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre forward_hook called\n",
      "forward hook called\n"
     ]
    }
   ],
   "source": [
    "##4.12 register_pre_forward_hook\n",
    "def pf_hook(m,input):\n",
    "    print('pre forward_hook called')\n",
    "\n",
    "n2.register_forward_pre_hook(pf_hook)\n",
    "out2=n2(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['param1', 'buf1', 'conv1.weight', 'conv1.bias', 'conv1.param2'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##4.13 state_dict\n",
    "##以字典形式返回模块内所有的参数和buf\n",
    "##包括所有子类的参数和buf\n",
    "n2._modules['conv1'].register_parameter('param2',nn.Parameter(torch.randn(2)))\n",
    "d=n2.state_dict()\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2380,  0.2048],\n",
       "        [-0.3255,  0.1231]], requires_grad=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##4.14 转移或转型方法：to\n",
    "linear=nn.Linear(2,2)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2380,  0.2048],\n",
       "        [-0.3255,  0.1231]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##参数的类型转换\n",
    "linear.to(torch.double)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=2, bias=True)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu1=torch.device('cuda:0')\n",
    "linear.to(gpu1,dtype=torch.half,non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2380,  0.2047],\n",
       "        [-0.3254,  0.1231]],\n",
       "       device='cuda:0', dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=2, bias=True)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu=torch.device('cpu')\n",
    "linear.to(cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2380,  0.2047],\n",
       "        [-0.3254,  0.1231]], dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 4.14 zero_grad\n",
    "##把所有参数的梯度清零（grad.detach_()+grad.zero_()）\n",
    "##如果没有梯度，即为None，则不进行处理\n",
    "n2._parameters['param1'].grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2._modules['conv1']._parameters['param2'].grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.0036, -1.6693, -0.6486], requires_grad=True) \n",
      " Parameter containing:\n",
      "tensor([-0.9009, -1.4198], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "n2.zero_grad()\n",
    "print(n2._parameters['param1'],'\\n',n2._modules['conv1']._parameters['param2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Sequential容器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "顺序容器，按传入module的顺序，构建网络，此时默认以序号（从0开始）作为module的名称，内部按add_module进行添加.\n",
    "\n",
    "也可以用OrderedDict作为参数传入.内容同样用add_module添加."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "        0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=nn.Sequential(\n",
    "    nn.Conv2d(3,16,3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16,16,3,padding=1),\n",
    "    nn.ReLU()\n",
    ")\n",
    "s.apply(init)\n",
    "s._modules['0'].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Sequential(\n",
      "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      ")\n",
      "0 --> Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "1 --> ReLU()\n",
      "2 --> Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "3 --> ReLU()\n"
     ]
    }
   ],
   "source": [
    "for name,m in s.named_modules():\n",
    "    print(name,'-->',m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "##使用OrderedDict构建\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Sequential(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU()\n",
      ")\n",
      "conv1 --> Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "relu1 --> ReLU()\n",
      "conv2 --> Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "relu2 --> ReLU()\n"
     ]
    }
   ],
   "source": [
    "s2=nn.Sequential(OrderedDict({\n",
    "    'conv1':nn.Conv2d(3,16,3,padding=1),\n",
    "    'relu1':nn.ReLU(),\n",
    "    'conv2':nn.Conv2d(16,16,3,padding=1),\n",
    "    'relu2':nn.ReLU()\n",
    "    \n",
    "}))\n",
    "\n",
    "for name,m in s2.named_modules():\n",
    "    print(name,'-->',m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.ModuleList容器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以列表的方式维持子模块.\n",
    "\n",
    "- 初始化：None或者可迭代的推断.\n",
    "- 方法：\n",
    "    - append\n",
    "    - extend    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self,n):\n",
    "        super(Net2,self).__init__()\n",
    "        self.linears=nn.ModuleList([nn.Linear(i,i+1) for i in range(2,n)])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for i ,l in enumerate(self.linears):\n",
    "            x=self.linears[i](x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5733, -0.7317, -0.4228, -1.1397],\n",
       "        [ 0.6553, -0.7369, -0.3785, -1.3431]], grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=Net2(4)\n",
    "input=torch.randn(2,2)\n",
    "out=net(input)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Net2(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=3, bias=True)\n",
      "    (1): Linear(in_features=3, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "linears --> ModuleList(\n",
      "  (0): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (1): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n",
      "linears.0 --> Linear(in_features=2, out_features=3, bias=True)\n",
      "linears.1 --> Linear(in_features=3, out_features=4, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name,m in net.named_modules():\n",
    "    print(name,'-->',m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linears.0.weight --> Parameter containing:\n",
      "tensor([[-0.2193, -0.2656],\n",
      "        [-0.0845,  0.2926],\n",
      "        [ 0.5343, -0.5668]], requires_grad=True)\n",
      "linears.0.bias --> Parameter containing:\n",
      "tensor([ 0.5299, -0.4193, -0.4374], requires_grad=True)\n",
      "linears.1.weight --> Parameter containing:\n",
      "tensor([[-0.0166,  0.1842, -0.0968],\n",
      "        [ 0.0312,  0.5146,  0.1864],\n",
      "        [-0.5233, -0.0546, -0.0803],\n",
      "        [-0.5487,  0.1799,  0.4886]], requires_grad=True)\n",
      "linears.1.bias --> Parameter containing:\n",
      "tensor([ 0.4810, -0.3749, -0.1120,  0.0878], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name,p in net.named_parameters():\n",
    "    print(name,'-->',p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.ModuleDict容器类\n",
    "\n",
    "以字典的方式维持子模块.\n",
    "\n",
    "- 初始化：以字典或者可迭代推断进行.\n",
    "- 方法：\n",
    "    - 字典添加\n",
    "    - pop/clear\n",
    "    - items/keys/values\n",
    "    - update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net3(nn.Module):\n",
    "    def __init__(self,mapping=None):\n",
    "        super(Net3,self).__init__()\n",
    "        if mapping is not None and isinstance(mapping,dict):\n",
    "            self.layers=nn.ModuleDict(mapping)\n",
    "        \n",
    "        ##注意moduleDict会自动排序，不是OrderedDict\n",
    "        self.act=nn.ModuleDict({\n",
    "            'relu':nn.ReLU(),\n",
    "            'asigmoid':nn.Sigmoid()\n",
    "        })\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.layers['conv1'](x)\n",
    "        x=self.act['relu'](x)\n",
    "        x=self.layers['pool1'](x)\n",
    "        \n",
    "        return self.act['asigmoid'](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 8, 8])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers={'conv1':nn.Conv2d(3,16,3,padding=1),'pool1':nn.AdaptiveMaxPool2d(8)}\n",
    "net=Net3(layers)\n",
    "input=torch.randn(4,3,12,12)\n",
    "out=net(input)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.conv1.weight --> torch.Size([16, 3, 3, 3])\n",
      "layers.conv1.bias --> torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for name,p in net.named_parameters():\n",
    "    print(name,'-->',p.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Net3(\n",
      "  (layers): ModuleDict(\n",
      "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (pool1): AdaptiveMaxPool2d(output_size=8)\n",
      "  )\n",
      "  (act): ModuleDict(\n",
      "    (asigmoid): Sigmoid()\n",
      "    (relu): ReLU()\n",
      "  )\n",
      ")\n",
      "layers --> ModuleDict(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): AdaptiveMaxPool2d(output_size=8)\n",
      ")\n",
      "layers.conv1 --> Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "layers.pool1 --> AdaptiveMaxPool2d(output_size=8)\n",
      "act --> ModuleDict(\n",
      "  (asigmoid): Sigmoid()\n",
      "  (relu): ReLU()\n",
      ")\n",
      "act.asigmoid --> Sigmoid()\n",
      "act.relu --> ReLU()\n"
     ]
    }
   ],
   "source": [
    "for name,m in net.named_modules():\n",
    "    print(name,'-->',m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
