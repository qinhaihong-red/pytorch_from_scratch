{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一 距离函数\n",
    "## 1.1 余弦相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4.],\n",
      "        [5., 6., 7., 8.]]) \n",
      " tensor([[10., 11., 12., 13.],\n",
      "        [14., 15., 16., 17.]])\n"
     ]
    }
   ],
   "source": [
    "x1=torch.arange(1,9,dtype=torch.float).reshape(2,4)\n",
    "x2=torch.arange(10,18,dtype=torch.float).reshape(2,4)\n",
    "print(x1,'\\n',x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9481, 0.9952])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=nn.CosineSimilarity()\n",
    "m(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numerator=torch.dot(x1[0,:],x2[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denominator=torch.sqrt(torch.sum(x1[0,:]**2))*torch.sqrt(torch.sum(x2[0,:]**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9481)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerator/denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 空间距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18.0000, 18.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=nn.PairwiseDistance()##默认为欧几里得距离，p=2\n",
    "m(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x1.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二 损失函数\n",
    "## 2.1 L1损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.],\n",
       "        [5., 6., 7., 8.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.arange(8,dtype=torch.float).reshape(2,4)\n",
    "t=y+1\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn=nn.L1Loss()##默认使用elementwise_mean\n",
    "loss=loss_fn(t,y)\n",
    "loss##注意输出为一个标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn=nn.L1Loss(reduction='sum')\n",
    "loss=loss_fn(t,y)\n",
    "loss##输出为一个标量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 均方误差MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.6250)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn=nn.MSELoss()\n",
    "t=y+2\n",
    "t[1,1]=10\n",
    "loss_fn(t,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__输出的总是标量，没有batch_size维__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target=torch.empty(3).random_(2)\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 二分类交叉熵损失：BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8589,  1.0864, -1.0978], requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E=nn.BCELoss()\n",
    "m=nn.Sigmoid()\n",
    "y=torch.randn(3,dtype=torch.float,requires_grad=True)\n",
    "target=torch.empty(3).random_(2)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7024, 0.7477, 0.2502], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3250, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=E(m(y),target)##对y进行激活之后，再算交叉熵\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2341,  0.2492, -0.2499])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3250, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##手动计算\n",
    "r=-torch.log(m(y))\n",
    "r2=-torch.log(1-m(y))\n",
    "r3=r2[0]+r2[1]+r[2]\n",
    "r3/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 二分类交叉熵手动计算说明\n",
    "\n",
    "$E(w)=-\\sum_{n}^{N}(t_{n}\\log y_{n}+(1-t_{n})\\log (1-y_{n})) $\n",
    "\n",
    "$\\bar{E(w)}=\\frac{E(w)}{N}$\n",
    "\n",
    "- 对m(y)和1-m(y)分别取负对数，记为r,r2\n",
    "- 如果target[n]=1，则取r[n]，否则取r2[n]\n",
    "- 上述相加除N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 负对数似然损失NLLLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1.1 一维示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8861,  2.8642,  2.1634,  1.5570,  1.1928],\n",
       "        [-0.2726, -0.3854,  0.7054, -1.1143,  0.3424],\n",
       "        [-0.0046,  1.3266,  1.6912, -0.3976, -0.9664]], requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E=nn.NLLLoss()\n",
    "m=nn.LogSoftmax(dim=1)\n",
    "y=torch.randn(3,5,dtype=torch.float,requires_grad=True)##N*C，3个样本，5个分类\n",
    "target=torch.tensor([1,0,4])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.4250, -0.6746, -1.3755, -1.9819, -2.3461],\n",
       "        [-1.9218, -2.0346, -0.9438, -2.7635, -1.3068],\n",
       "        [-2.4242, -1.0931, -0.7285, -2.8173, -3.3860]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss=E(m(y),target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9941, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0015, -0.1636,  0.0842,  0.0459,  0.0319],\n",
       "        [-0.2846,  0.0436,  0.1297,  0.0210,  0.0902],\n",
       "        [ 0.0295,  0.1117,  0.1609,  0.0199, -0.3221]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9941, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##手动计算损失\n",
    "r=m(y)[0,1]+m(y)[1,0]+m(y)[2,4]\n",
    "-r/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1.2 多分类交叉熵手动计算说明\n",
    "$E(w)=-\\sum_{n}^{N}t_{n}\\log y_{n}$\n",
    "\n",
    "$\\bar{E(w)}=\\frac{E(w)}{N}$\n",
    "\n",
    "假设 target=torch.tensor([1,0,4])，因随机数每次生成可能不同：\n",
    "\n",
    "- 第一个样本属于第1类，即$t_{1}=1而t_{k\\ne1}=0$，因此只取下标为[0,1]的元素即可\n",
    "- 同理，第二、三个样本，只取[1,0]、[2,4]即可\n",
    "- 三者相加取负数除3即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.2 二维示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 3., 0., 0., 3., 3., 3., 3.],\n",
       "         [3., 2., 1., 1., 2., 3., 2., 2.],\n",
       "         [3., 2., 3., 2., 3., 2., 1., 0.],\n",
       "         [2., 2., 3., 0., 2., 3., 2., 0.],\n",
       "         [3., 0., 3., 1., 0., 3., 3., 0.],\n",
       "         [1., 3., 3., 2., 3., 0., 2., 2.],\n",
       "         [3., 3., 2., 0., 1., 1., 1., 2.],\n",
       "         [1., 2., 2., 3., 1., 1., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 1., 2., 0., 3., 1., 2.],\n",
       "         [2., 2., 3., 1., 3., 1., 3., 3.],\n",
       "         [2., 1., 3., 1., 0., 3., 3., 1.],\n",
       "         [2., 3., 2., 1., 1., 0., 3., 1.],\n",
       "         [0., 3., 3., 1., 2., 1., 3., 2.],\n",
       "         [2., 2., 0., 3., 1., 1., 1., 3.],\n",
       "         [0., 1., 2., 1., 1., 2., 1., 1.],\n",
       "         [3., 1., 2., 2., 0., 3., 0., 0.]],\n",
       "\n",
       "        [[0., 2., 0., 0., 2., 0., 1., 2.],\n",
       "         [2., 2., 0., 1., 3., 3., 2., 0.],\n",
       "         [0., 1., 2., 0., 3., 3., 1., 1.],\n",
       "         [2., 2., 2., 0., 3., 0., 1., 2.],\n",
       "         [0., 2., 3., 2., 2., 1., 2., 3.],\n",
       "         [3., 0., 2., 1., 0., 0., 1., 0.],\n",
       "         [1., 1., 3., 2., 2., 3., 0., 0.],\n",
       "         [3., 3., 2., 2., 2., 2., 2., 3.]],\n",
       "\n",
       "        [[2., 3., 1., 3., 1., 0., 2., 1.],\n",
       "         [2., 0., 2., 2., 1., 3., 2., 1.],\n",
       "         [2., 3., 3., 0., 3., 3., 2., 0.],\n",
       "         [1., 1., 0., 2., 1., 1., 3., 1.],\n",
       "         [1., 0., 0., 2., 0., 1., 0., 3.],\n",
       "         [2., 1., 3., 0., 0., 1., 2., 2.],\n",
       "         [2., 1., 0., 1., 3., 0., 0., 3.],\n",
       "         [0., 3., 2., 1., 1., 2., 3., 3.]],\n",
       "\n",
       "        [[3., 3., 1., 2., 3., 0., 2., 2.],\n",
       "         [2., 3., 2., 1., 2., 3., 0., 0.],\n",
       "         [2., 2., 2., 3., 2., 3., 1., 2.],\n",
       "         [1., 0., 0., 1., 3., 1., 3., 0.],\n",
       "         [0., 0., 0., 3., 0., 2., 0., 2.],\n",
       "         [0., 2., 0., 0., 3., 3., 2., 0.],\n",
       "         [3., 2., 2., 2., 1., 2., 1., 0.],\n",
       "         [2., 3., 3., 3., 0., 1., 0., 2.]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N,C=5,4 ##5个样本，4个分类\n",
    "E=nn.NLLLoss()\n",
    "X=torch.randn(N,16,10,10)##4个样本，16个通道，H*W=10x10\n",
    "target=torch.empty(N,8,8).random_(0,C)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 8, 8])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##注意卷积核的形状\n",
    "m=nn.Conv2d(16,C,3)#输入16个通道，输出C=4个通道，核尺寸为3，则输出后尺寸:[(10-3)/1]+1=8\n",
    "y=m(X)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0063, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target=target.to(torch.long)\n",
    "loss=E(y,target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 交叉熵损失CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个损失函数相当于 LogSoftmax+NLLLoss 的混合，即对于输入数据，不必再进行LogSoftmax的变换，直接计算交叉熵损失即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 2])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E=nn.CrossEntropyLoss()\n",
    "X=torch.randn(3,5)\n",
    "target=torch.empty(3,dtype=torch.long).random_(5)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7148)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=E(X,target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9942)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=torch.tensor([[-1.8861,  2.8642,  2.1634,  1.5570,  1.1928],\n",
    "        [-0.2726, -0.3854,  0.7054, -1.1143,  0.3424],\n",
    "        [-0.0046,  1.3266,  1.6912, -0.3976, -0.9664]])\n",
    "\n",
    "target=torch.tensor([1,0,4])\n",
    "\n",
    "E(X,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__在使用相同数据X的情况下，这个结果与2.4.1.1的输出结果一致__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 KL散度损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0686)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "E=nn.KLDivLoss()\n",
    "log_prob1=F.log_softmax(torch.randn(5,10),dim=1)\n",
    "prob2=F.softmax(torch.randn(5,10),dim=1)\n",
    "E(log_prob1,prob2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 带sigmoid激活的二分类交叉熵损失 BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与BCELoss相比，自动添加了sigmoid激活"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3250)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E=nn.BCEWithLogitsLoss()\n",
    "y=torch.tensor([ 0.8589,  1.0864, -1.0978])\n",
    "target=torch.tensor([0., 0., 1.])\n",
    "E(y,target)##注意变换的变量在前，标签在后，顺序不能乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3250)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E2=nn.BCELoss()\n",
    "m=nn.Sigmoid()\n",
    "E2(m(y),target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 其他损失\n",
    "smoothl1loss等，详见：https://pytorch.org/docs/stable/nn.html#smoothl1loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
