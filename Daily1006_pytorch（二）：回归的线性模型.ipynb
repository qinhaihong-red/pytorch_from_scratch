{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归的线性模型\n",
    "\n",
    "- 使用pytorch求解线性回归问题（超定方程的最小二乘解）\n",
    "    - 使用pytorch.gels（对应的是正规方程的解析闭式解）\n",
    "    - 使用pytorc.nn.Linear，相当于对gels的封装，例如不用为设计矩阵加1等\n",
    "    - 使用优化器. 当超定方程过大，无法一次加载，可使用基于梯度下降的优化算法进行迭代求数值解\n",
    "   \n",
    "\n",
    "- 损失函数的分类\n",
    "    - L1\n",
    "    - L2\n",
    "    - SmoothL1\n",
    " \n",
    "\n",
    "- 数据的标准化\n",
    "    - 标准化的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4.6667],\n",
       "        [  2.6667],\n",
       "        [-12.0000]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "##1.1-多元线性回归\n",
    "X = torch.tensor([[1,1,1,],[2,3,1],[3,5,1],[4,2,1],[5,4,1]],dtype=torch.float)\n",
    "y = torch.tensor([-10,12,14,16,18],dtype=torch.float)\n",
    "w,_=torch.gels(y,X)##这里的顺序不能错\n",
    "w[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4.6667,   2.5333],\n",
       "        [  2.6667,   0.5333],\n",
       "        [-12.0000,   3.0000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##1.2-多变量线性回归\n",
    "Y=torch.tensor([[-10,3],[12,14],[14,12],[16,16],[18,16]],dtype=torch.float)\n",
    "W,_=torch.gels(Y,X)\n",
    "W[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##2.损失函数\n",
    "E=torch.nn.MSELoss()\n",
    "y=torch.arange(5,requires_grad=True,dtype=torch.float)\n",
    "t=torch.ones(5)\n",
    "\n",
    "##2.1-得到均方误差损失\n",
    "loss=E(y,t)##注意这里的顺序不能错，t属于标签数据，y是预测数据，要对y相关的变量求梯度\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4000, grad_fn=<L1LossBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##2.2-L1 Loss\n",
    "E_L1=torch.nn.L1Loss()\n",
    "loss2=E_L1(y,t)\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SmoothL1LossBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##2.3-SmoothL1 Loss\n",
    "E_SL1=torch.nn.SmoothL1Loss()\n",
    "loss3=E_SL1(y,t)\n",
    "loss3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [2., 3., 1.],\n",
       "        [3., 5., 1.],\n",
       "        [4., 2., 1.],\n",
       "        [5., 4., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##3.使用优化器求解回归问题的解\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.,  12.,  14.,  16.,  18.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([-10,12,14,16,18],dtype=torch.float)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次：loss=203.816,w=[0.001 0.001 0.001],grad_w=[-84. -80. -20.]\n",
      "第1000次：loss=86.777,w=[0.864 0.855 0.822],grad_w=[-43.001 -39.018  -8.054]\n",
      "第2000次：loss=53.616,w=[1.444 1.395 1.19 ],grad_w=[-17.205 -13.303  -0.59 ]\n",
      "第3000次：loss=47.502,w=[1.784 1.634 0.957],grad_w=[-6.32  -2.619  2.425]\n",
      "第4000次：loss=44.285,w=[2.032 1.702 0.255],grad_w=[-3.725 -0.383  2.915]\n",
      "第5000次：loss=40.87,w=[ 2.305  1.713 -0.619],grad_w=[-2.743  0.072  2.87 ]\n",
      "第6000次：loss=37.534,w=[ 2.62   1.685 -1.545],grad_w=[-1.932  0.2    2.74 ]\n",
      "第7000次：loss=34.523,w=[ 2.951  1.636 -2.48 ],grad_w=[-1.233  0.137  2.564]\n",
      "第8000次：loss=31.943,w=[ 3.26   1.603 -3.407],grad_w=[-0.672  0.015  2.362]\n",
      "第9000次：loss=29.763,w=[ 3.504  1.627 -4.322],grad_w=[-0.307 -0.061  2.141]\n",
      "第10000次：loss=27.898,w=[ 3.679  1.713 -5.221],grad_w=[-0.137 -0.068  1.908]\n",
      "第11000次：loss=26.302,w=[ 3.816  1.83  -6.103],grad_w=[-0.073 -0.046  1.668]\n",
      "第12000次：loss=24.956,w=[ 3.943  1.95  -6.964],grad_w=[-0.043 -0.028  1.429]\n",
      "第13000次：loss=23.85,w=[ 4.064  2.069 -7.803],grad_w=[-0.025 -0.017  1.193]\n",
      "第14000次：loss=22.972,w=[ 4.181  2.184 -8.613],grad_w=[-0.015 -0.01   0.964]\n",
      "第15000次：loss=22.309,w=[ 4.292  2.294 -9.386],grad_w=[-0.008 -0.006  0.745]\n",
      "第16000次：loss=21.844,w=[  4.396   2.397 -10.109],grad_w=[-0.005 -0.003  0.539]\n",
      "第17000次：loss=21.553,w=[  4.489   2.49  -10.761],grad_w=[-0.003 -0.002  0.354]\n",
      "第18000次：loss=21.402,w=[  4.568   2.568 -11.308],grad_w=[-0.001 -0.001  0.197]\n",
      "第19000次：loss=21.346,w=[  4.625   2.625 -11.708],grad_w=[-0.    -0.     0.083]\n",
      "第20000次：loss=21.334,w=[  4.656   2.656 -11.925],grad_w=[-0.    -0.     0.021]\n",
      "第21000次：loss=21.333,w=[  4.666   2.666 -11.992],grad_w=[0.    0.    0.002]\n",
      "第22000次：loss=21.333,w=[  4.667   2.667 -12.   ],grad_w=[-0. -0.  0.]\n",
      "第23000次：loss=21.333,w=[  4.667   2.667 -12.   ],grad_w=[-0. -0.  0.]\n",
      "第24000次：loss=21.333,w=[  4.667   2.667 -12.   ],grad_w=[-0. -0. -0.]\n",
      "第25000次：loss=21.333,w=[  4.667   2.667 -12.   ],grad_w=[0. 0. 0.]\n",
      "第26000次：loss=21.333,w=[  4.667   2.667 -12.   ],grad_w=[-0. -0. -0.]\n",
      "第27000次：loss=21.333,w=[  4.667   2.667 -12.   ],grad_w=[-0. -0. -0.]\n",
      "第28000次：loss=21.333,w=[  4.667   2.667 -12.   ],grad_w=[0.001 0.001 0.   ]\n",
      "第29000次：loss=21.333,w=[  4.667   2.667 -12.   ],grad_w=[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "w=torch.zeros(3,requires_grad=True,dtype=torch.float)\n",
    "E=torch.nn.MSELoss()\n",
    "opt=torch.optim.Adam([w])\n",
    "loss=E(torch.mv(X,w),y)\n",
    "\n",
    "for i in range(30000):\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    loss=E(torch.mv(X,w),y)\n",
    "    \n",
    "    if i%1000 == 0:\n",
    "        print('第{}次：loss={},w={},grad_w={}'.format(i,np.round(loss.tolist(),3),  \n",
    "                                                   np.round(w.tolist(),3),\n",
    "                                                   np.round(w.grad.tolist(),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [2., 3.],\n",
       "        [3., 5.],\n",
       "        [4., 2.],\n",
       "        [5., 4.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##4.使用torch.nn.Linear求解线性回归问题\n",
    "X=X[:,:2]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=y.reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次：loss=162.092,w=Parameter containing:\n",
      "tensor([[0.0479, 0.5082]], requires_grad=True),grad_w=[[-72.6204833984375, -67.69975280761719]],b=Parameter containing:\n",
      "tensor([0.0349], requires_grad=True),grad_b=[-16.607486724853516]\n",
      "第1000次：loss=69.947,w=Parameter containing:\n",
      "tensor([[0.8876, 1.3322]], requires_grad=True),grad_w=[[-32.99868392944336, -28.10956573486328]],b=Parameter containing:\n",
      "tensor([0.8101], requires_grad=True),grad_b=[-5.069876670837402]\n",
      "第2000次：loss=50.266,w=Parameter containing:\n",
      "tensor([[1.4065, 1.7768]], requires_grad=True),grad_w=[[-11.610899925231934, -6.8700408935546875]],b=Parameter containing:\n",
      "tensor([0.9874], requires_grad=True),grad_b=[1.0705972909927368]\n",
      "第3000次：loss=46.204,w=Parameter containing:\n",
      "tensor([[1.7154, 1.9131]], requires_grad=True),grad_w=[[-5.117435932159424, -0.7214572429656982]],b=Parameter containing:\n",
      "tensor([0.4808], requires_grad=True),grad_b=[2.732027053833008]\n",
      "第4000次：loss=42.606,w=Parameter containing:\n",
      "tensor([[2.0039, 1.9214]], requires_grad=True),grad_w=[[-3.631044387817383, 0.2045377492904663]],b=Parameter containing:\n",
      "tensor([-0.3574], requires_grad=True),grad_b=[2.8372230529785156]\n",
      "第5000次：loss=38.959,w=Parameter containing:\n",
      "tensor([[2.3413, 1.8747]], requires_grad=True),grad_w=[[-2.6731815338134766, 0.39447611570358276]],b=Parameter containing:\n",
      "tensor([-1.2792], requires_grad=True),grad_b=[2.7378313541412354]\n",
      "第6000次：loss=35.595,w=Parameter containing:\n",
      "tensor([[2.7141, 1.7885]], requires_grad=True),grad_w=[[-1.824765920639038, 0.32482069730758667]],b=Parameter containing:\n",
      "tensor([-2.2174], requires_grad=True),grad_b=[2.581014394760132]\n",
      "第7000次：loss=32.716,w=Parameter containing:\n",
      "tensor([[3.0828, 1.6998]], requires_grad=True),grad_w=[[-1.0868760347366333, 0.14800667762756348]],b=Parameter containing:\n",
      "tensor([-3.1508], requires_grad=True),grad_b=[2.394484758377075]\n",
      "第8000次：loss=30.347,w=Parameter containing:\n",
      "tensor([[3.3950, 1.6608]], requires_grad=True),grad_w=[[-0.5330620408058167, -0.0008027553558349609]],b=Parameter containing:\n",
      "tensor([-4.0730], requires_grad=True),grad_b=[2.188852310180664]\n",
      "第9000次：loss=28.377,w=Parameter containing:\n",
      "tensor([[3.6192, 1.7018]], requires_grad=True),grad_w=[[-0.2251080870628357, -0.05961465835571289]],b=Parameter containing:\n",
      "tensor([-4.9804], requires_grad=True),grad_b=[1.9651896953582764]\n",
      "第10000次：loss=26.703,w=Parameter containing:\n",
      "tensor([[3.7760, 1.8020]], requires_grad=True),grad_w=[[-0.10493403673171997, -0.05307722091674805]],b=Parameter containing:\n",
      "tensor([-5.8695], requires_grad=True),grad_b=[1.729252576828003]\n",
      "第11000次：loss=25.289,w=Parameter containing:\n",
      "tensor([[3.9076, 1.9201]], requires_grad=True),grad_w=[[-0.05874258279800415, -0.03375363349914551]],b=Parameter containing:\n",
      "tensor([-6.7381], requires_grad=True),grad_b=[1.4904186725616455]\n",
      "第12000次：loss=24.119,w=Parameter containing:\n",
      "tensor([[4.0317, 2.0387]], requires_grad=True),grad_w=[[-0.03434646129608154, -0.020284652709960938]],b=Parameter containing:\n",
      "tensor([-7.5841], requires_grad=True),grad_b=[1.254132866859436]\n",
      "第13000次：loss=23.182,w=Parameter containing:\n",
      "tensor([[4.1504, 2.1544]], requires_grad=True),grad_w=[[-0.020051121711730957, -0.011999130249023438]],b=Parameter containing:\n",
      "tensor([-8.4029], requires_grad=True),grad_b=[1.0234019756317139]\n",
      "第14000次：loss=22.464,w=Parameter containing:\n",
      "tensor([[4.2635, 2.2658]], requires_grad=True),grad_w=[[-0.011594772338867188, -0.0069942474365234375]],b=Parameter containing:\n",
      "tensor([-9.1872], requires_grad=True),grad_b=[0.8012237548828125]\n",
      "第15000次：loss=21.948,w=Parameter containing:\n",
      "tensor([[4.3695, 2.3708]], requires_grad=True),grad_w=[[-0.006549477577209473, -0.003970623016357422]],b=Parameter containing:\n",
      "tensor([-9.9252], requires_grad=True),grad_b=[0.5915020108222961]\n",
      "第16000次：loss=21.614,w=Parameter containing:\n",
      "tensor([[4.4660, 2.4667]], requires_grad=True),grad_w=[[-0.003529071807861328, -0.0021333694458007812]],b=Parameter containing:\n",
      "tensor([-10.5983], requires_grad=True),grad_b=[0.3998699188232422]\n",
      "第17000次：loss=21.43,w=Parameter containing:\n",
      "tensor([[4.5489, 2.5493]], requires_grad=True),grad_w=[[-0.0017670392990112305, -0.0010733604431152344]],b=Parameter containing:\n",
      "tensor([-11.1772], requires_grad=True),grad_b=[0.2348167896270752]\n",
      "第18000次：loss=21.354,w=Parameter containing:\n",
      "tensor([[4.6123, 2.6125]], requires_grad=True),grad_w=[[-0.0007368326187133789, -0.0004467964172363281]],b=Parameter containing:\n",
      "tensor([-11.6203], requires_grad=True),grad_b=[0.10843360424041748]\n",
      "第19000次：loss=21.335,w=Parameter containing:\n",
      "tensor([[4.6503, 2.6504]], requires_grad=True),grad_w=[[-0.0002200603485107422, -0.00013637542724609375]],b=Parameter containing:\n",
      "tensor([-11.8859], requires_grad=True),grad_b=[0.03259730339050293]\n",
      "第20000次：loss=21.333,w=Parameter containing:\n",
      "tensor([[4.6644, 2.6644]], requires_grad=True),grad_w=[[-3.230571746826172e-05, -1.9550323486328125e-05]],b=Parameter containing:\n",
      "tensor([-11.9840], requires_grad=True),grad_b=[0.00457155704498291]\n",
      "第21000次：loss=21.333,w=Parameter containing:\n",
      "tensor([[4.6666, 2.6666]], requires_grad=True),grad_w=[[-3.4570693969726562e-06, -3.337860107421875e-06]],b=Parameter containing:\n",
      "tensor([-11.9994], requires_grad=True),grad_b=[0.000171661376953125]\n",
      "第22000次：loss=21.333,w=Parameter containing:\n",
      "tensor([[4.6667, 2.6667]], requires_grad=True),grad_w=[[3.731250762939453e-05, 3.719329833984375e-05]],b=Parameter containing:\n",
      "tensor([-11.9999], requires_grad=True),grad_b=[2.944469451904297e-05]\n",
      "第23000次：loss=21.333,w=Parameter containing:\n",
      "tensor([[4.6667, 2.6666]], requires_grad=True),grad_w=[[0.0006793737411499023, 0.0006928443908691406]],b=Parameter containing:\n",
      "tensor([-12.0000], requires_grad=True),grad_b=[0.0001976490020751953]\n",
      "第24000次：loss=21.333,w=Parameter containing:\n",
      "tensor([[4.6667, 2.6667]], requires_grad=True),grad_w=[[0.00039517879486083984, 0.0003981590270996094]],b=Parameter containing:\n",
      "tensor([-12.], requires_grad=True),grad_b=[0.00011336803436279297]\n",
      "第25000次：loss=21.333,w=Parameter containing:\n",
      "tensor([[4.6666, 2.6666]], requires_grad=True),grad_w=[[0.0008486509323120117, 0.0008487701416015625]],b=Parameter containing:\n",
      "tensor([-12.0000], requires_grad=True),grad_b=[0.00024235248565673828]\n",
      "第26000次：loss=21.333,w=Parameter containing:\n",
      "tensor([[4.6667, 2.6667]], requires_grad=True),grad_w=[[-0.00045549869537353516, -0.0004572868347167969]],b=Parameter containing:\n",
      "tensor([-12.0000], requires_grad=True),grad_b=[-0.0001308917999267578]\n",
      "第27000次：loss=21.333,w=Parameter containing:\n",
      "tensor([[4.6667, 2.6667]], requires_grad=True),grad_w=[[-5.137920379638672e-05, -5.3882598876953125e-05]],b=Parameter containing:\n",
      "tensor([-12.0000], requires_grad=True),grad_b=[-1.5735626220703125e-05]\n",
      "第28000次：loss=21.333,w=Parameter containing:\n",
      "tensor([[4.6667, 2.6667]], requires_grad=True),grad_w=[[0.0005075931549072266, 0.0005106925964355469]],b=Parameter containing:\n",
      "tensor([-12.0000], requires_grad=True),grad_b=[0.00014579296112060547]\n",
      "第29000次：loss=21.333,w=Parameter containing:\n",
      "tensor([[4.6667, 2.6667]], requires_grad=True),grad_w=[[0.0001938343048095703, 0.0001926422119140625]],b=Parameter containing:\n",
      "tensor([-12.0000], requires_grad=True),grad_b=[5.543231964111328e-05]\n"
     ]
    }
   ],
   "source": [
    "##生成一个全连接层：2个输入，1个输出\n",
    "##或者说是该层有1个神经元，这个神经元有2个突触\n",
    "\n",
    "fc=torch.nn.Linear(2,1)##权值包含在这个fc中\n",
    "E=torch.nn.MSELoss()\n",
    "opt=torch.optim.Adam(fc.parameters())\n",
    "\n",
    "pred=fc(X)\n",
    "loss=E(fc(X),y)\n",
    "\n",
    "w,b=fc.parameters()\n",
    "\n",
    "for i in range(30000):\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    loss=E(fc(X),y)\n",
    "    \n",
    "    \n",
    "    if i%1000 ==0:\n",
    "        print('第{}次：loss={},w={},grad_w={},b={},grad_b={}'.format(i,np.round( loss.tolist() ,3)\n",
    "                                                                  ,w,w.grad.tolist(),b,b.grad.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=torch.arange(1,6,3)\n",
    "v.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=v.double()\n",
    "i.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([.1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w=torch.tensor([2,2,2],dtype=torch.float,requires_grad=True)\n",
    "y=x*w\n",
    "z=y.sum()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable as Var\n",
    "xv=Var(x,requires_grad=True)\n",
    "wv=Var(w)\n",
    "yv=xv*wv\n",
    "zv=yv.sum()\n",
    "zv.backward(retain_graph=True)\n",
    "xv.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xv.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(zv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SumBackward0 at 0x2afbdc60748>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zv.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zv.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ThMulBackward at 0x2afbdbce978>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yv.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<AccumulateGrad at 0x2afbdbd94a8>, 0), (None, 0))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yv.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zv.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<ThMulBackward at 0x2afbdbce978>, 0),)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zv.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(zv.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xv.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(xv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], dtype=torch.int32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=torch.tensor([1,2,3],dtype=torch.int)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.IntTensor'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.FloatTensor)==type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-143-09de6b29cf52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mzv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3-4.4.0\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3-4.4.0\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "zv.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xv.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "yv.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(xv.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
